{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trackingML DL with GlueX Fall 2018 data part 2\n",
    "\n",
    "In this notebook I take the example from the toy4 problem (=ML Challenge 2) and make a complete example for fitting the state vector. This is not the final model and training that we want, but it lays out *a* working example that shows how to deal with a lot of technical details.\n",
    "\n",
    "In here I make a model that implements a common input section and then 5 separate \"legs\", one for each of the 5 state vector parameters. The final layer merges these all back into a single, 5-parameter output layer. Note that this does not try to include covariance values in the model output. I'll deal with that in the next part.\n",
    "\n",
    "A nice thing about this setup is that each parameter has its own dedicated part of the model. Thus, if one parameter needs more layers or a certain structure, it can be fine tuned without affecting the others.\n",
    "\n",
    "One major improvement here over the toy4 model is that the input data comes in CSV format that removes the need for a generator. This greatly simplifies the code at the expense of making the input features file 2.5 times bigger than raw EVIO data file (50GB vs. 20GB).\n",
    "\n",
    "Something else implemented here from before is the use of a customized layer that calculates a weighted average. In earlier toy model tests this showed significant improvement in accuracy. The hypothesis is that it includes a division operation which cannot be done with normal layers expect under special conditions. Division could be mimiced using lots of layers in a deeper network, but that would mean more parameters and longer training.\n",
    "\n",
    "This first section just reads in a portion of the training files and sets a few parameters used later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of input features per track: 5835\n",
      "Number of tracks read: 1000  ( 0.06% of total )\n",
      "Label Names: \n",
      "              event\n",
      "              q_over_pt, phi, tanl, D, z\n",
      "              cov_00, cov_01, cov_02, cov_03, cov_04\n",
      "              cov_11, cov_12, cov_13, cov_14, cov_22\n",
      "              cov_23, cov_24, cov_33, cov_34, cov_44\n",
      "              chisq, Ndof\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, Input, Lambda\n",
    "from tensorflow.keras.optimizers import SGD, Adamax, Adadelta\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.losses\n",
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_FILE  = '/home/davidl/work2/2020.04.30.trackingML/trackingML_features.csv'\n",
    "LABELS_FILE = '/home/davidl/work2/2020.04.30.trackingML/trackingML_labels.csv'\n",
    "MAX_TRACKS  = 1000  # Number of tracks(lines) to read from training file\n",
    "\n",
    "GPUS   = 0  # 0=force CPU, otherwise, the number of GPUs to use\n",
    "Nouts  = 60 # For Lamda layers that use MyWeightedAvg()\n",
    "\n",
    "#------------- Define output bin values and ranges\n",
    "DMIN   = -12.0\n",
    "DMAX   =  12.0\n",
    "D_BINSIZE = (DMAX-DMIN)/Nouts\n",
    "\n",
    "PHIMIN   = -12.0\n",
    "PHIMAX   =  12.0\n",
    "PHI_BINSIZE = (PHIMAX-PHIMIN)/Nouts\n",
    "\n",
    "QOVERPt_MIN   = -12.0\n",
    "QOVERPt_MAX   =  12.0\n",
    "QOVERPt_BINSIZE = (QOVERPt_MAX-QOVERPt_MIN)/Nouts\n",
    "\n",
    "TANLMIN   = -10.0\n",
    "TANLMAX   =  10.0\n",
    "TANL_BINSIZE = (TANLMAX-TANLMIN)/Nouts\n",
    "\n",
    "ZMIN   = -10.0\n",
    "ZMAX   =  10.0\n",
    "Z_BINSIZE = (ZMAX-ZMIN)/Nouts\n",
    "#--------------\n",
    "\n",
    "# Get fraction of features file read so we can estimate fraction of data we're using\n",
    "percent_read = 0.0;\n",
    "with open(TRAIN_FILE) as f:\n",
    "    f.readline()  # skip header\n",
    "    for i in range(100): percent_read = percent_read + float(len(f.readline()) + 1)\n",
    "    percent_read = 100.0*percent_read/os.path.getsize(TRAIN_FILE)*MAX_TRACKS/100.0\n",
    "\n",
    "# NOTE: Only reading first few rows for now since it takes a long time to read entire file\n",
    "df       = pd.read_csv(TRAIN_FILE  , nrows=MAX_TRACKS)\n",
    "labelsdf = pd.read_csv(LABELS_FILE , nrows=MAX_TRACKS)\n",
    "\n",
    "NINPUTS = len(df.columns)\n",
    "\n",
    "print('\\n\\nNumber of input features per track: %d' % NINPUTS)\n",
    "print('Number of tracks read: %d  ( %3.2f%% of total )' % (len(df.index), percent_read))\n",
    "print('Label Names: ')\n",
    "print('              ' + ', '.join(labelsdf.columns[0:1]))\n",
    "print('              ' + ', '.join(labelsdf.columns[1:6]))\n",
    "print('              ' + ', '.join(labelsdf.columns[6:11]))\n",
    "print('              ' + ', '.join(labelsdf.columns[11:16]))\n",
    "print('              ' + ', '.join(labelsdf.columns[16:21]))\n",
    "print('              ' + ', '.join(labelsdf.columns[21:]))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layer Definition\n",
    "\n",
    "This section defines the layers of the model. It actually just defines some procedures that aren't executed until a couple of cells later. That when any errors will come up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# DefineCommonModel\n",
    "#-----------------------------------------------------\n",
    "def DefineCommonModel(inputs):\n",
    "    x = Flatten(name='top_layer1')(inputs)\n",
    "    x = Dense(int(Nouts*5), name='common_layer1', activation='linear', kernel_initializer=\"glorot_uniform\")(x)\n",
    "    return x\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# DefineDModel\n",
    "#-----------------------------------------------------\n",
    "def DefineDModel(inputs):\n",
    "    x = Dense(Nouts, name='D_output_dist', activation='relu', kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "    x = Lambda(MyWeightedAvg, output_shape=(1,), name='D_output', arguments={'binsize':D_BINSIZE, 'xmin':DMIN})(x)\n",
    "    return x\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# DefinePhiModel\n",
    "#-----------------------------------------------------\n",
    "def DefinePhiModel(inputs):\n",
    "    x = Dense(Nouts, name='phi_output_dist', activation='relu', kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "    x = Lambda(MyWeightedAvg, output_shape=(1,), name='phi_output', arguments={'binsize':PHI_BINSIZE, 'xmin':PHIMIN})(x)\n",
    "    return x\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# Define_q_over_pt_Model\n",
    "#-----------------------------------------------------\n",
    "def Define_q_over_pt_Model(inputs):\n",
    "    x = Dense(Nouts, name='q_over_pt_output_dist', activation='relu', kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "    x = Lambda(MyWeightedAvg, output_shape=(1,), name='q_over_pt_output', arguments={'binsize':QOVERPt_BINSIZE, 'xmin':QOVERPt_MIN})(x)\n",
    "    return x\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# Define_tanl_Model\n",
    "#-----------------------------------------------------\n",
    "def Define_tanl_Model(inputs):\n",
    "    x = Dense(Nouts, name='tanl_output_dist', activation='relu', kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "    x = Lambda(MyWeightedAvg, output_shape=(1,), name='tanl_output', arguments={'binsize':TANL_BINSIZE, 'xmin':TANLMIN})(x)\n",
    "    return x\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# DefineZModel\n",
    "#-----------------------------------------------------\n",
    "def DefineZModel(inputs):\n",
    "    x = Dense(Nouts, name='z_output_dist', activation='relu', kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "    x = Lambda(MyWeightedAvg, output_shape=(1,), name='z_output', arguments={'binsize':Z_BINSIZE, 'xmin':ZMIN})(x)\n",
    "    return x\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# DefineCommonOutput\n",
    "#-----------------------------------------------------\n",
    "def DefineCommonOutput(inputs):\n",
    "    x = tf.keras.layers.concatenate( inputs )\n",
    "    x = Dense(5, name='outputs', activation='relu', kernel_initializer=\"glorot_uniform\")(x)\n",
    "    return x\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# MyWeightedAvg\n",
    "#\n",
    "# This is used by the final Lambda layer in each branch\n",
    "# of the network. It defines the formula for calculating\n",
    "# the weighted average of the inputs from the previous\n",
    "# layer.\n",
    "#-----------------------------------------------------\n",
    "def MyWeightedAvg(inputs, binsize, xmin):\n",
    "    ones = K.ones_like(inputs[0,:])                       # [1, 1, 1, 1....]   (size Nouts)\n",
    "    idx  = K.cumsum(ones)                                 # [1, 2, 3, 4....]   (size Nouts)\n",
    "    norm = K.sum(inputs, axis=1, keepdims=True)           # normalization of all outputs by batch. shape is 1D array of size batch (n.b. keepdims=True is critical!)\n",
    "    wsum = K.sum(idx*inputs, axis=1, keepdims=True)/norm  # array of size batch with weighted avg. of mean in units of bins (n.b. keepdims=True is critical!)\n",
    "    output = (binsize*(wsum-0.5)) + xmin                  # convert from bins to physical units (shape batch,1)\n",
    "\n",
    "    print('MyWeightedAvg:')\n",
    "    print('       binsize = %f' % binsize)\n",
    "    print('          xmin = %f' % xmin)\n",
    "    print('   input shape = %s' % str(inputs.shape))\n",
    "    print('  output shape = %s' % str(output.shape))\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# DefineModel\n",
    "#-----------------------------------------------------\n",
    "# This is used to define the model. It is only called if no model\n",
    "# file is found in the model_checkpoints directory.\n",
    "def DefineModel():\n",
    "\n",
    "    # If GPUS==0 this will force use of CPU, even if GPUs are present\n",
    "    # If GPUS>1 this will force the CPU to serve as orchestrator\n",
    "    # If GPUS==1 this will do nothing, allowing GPU to act as its own orchestrator\n",
    "    if GPUS!=1: tf.device('/cpu:0')\n",
    "\n",
    "    # Here we build the network model.\n",
    "    # This model is made of multiple parts. The first handles the\n",
    "    # inputs and identifies common features. The rest are branches with\n",
    "    # each determining an output parameter from those features.\n",
    "    inputs         = Input(shape=(NINPUTS,), name='image_inputs')\n",
    "    commonmodel    = DefineCommonModel(inputs)\n",
    "    Dmodel         = DefinePhiModel(         commonmodel )\n",
    "    phimodel       = DefineDModel(           commonmodel )\n",
    "    q_over_ptmodel = Define_q_over_pt_Model( commonmodel )\n",
    "    tanlmodel      = Define_tanl_Model(      commonmodel )\n",
    "    zmodel         = DefineZModel(           commonmodel )\n",
    "    commonoutput   = DefineCommonOutput([Dmodel, phimodel, q_over_ptmodel, tanlmodel, zmodel])\n",
    "    model          = Model(inputs=inputs, outputs=commonoutput)\n",
    "    #model          = Model(inputs=inputs, outputs=[Dmodel, phimodel, q_over_ptmodel, tanlmodel, zmodel])\n",
    "    model.summary()\n",
    "\n",
    "    # Here we specify a different loss function for every output branch.\n",
    "    # We also specify a weight for each branch. The weights allow us to \n",
    "    # specify that it is more important to minimize certain losses more\n",
    "    # than others.\n",
    "    sigma_D = 0.011    # placeholder\n",
    "    sigma_phi = 0.011  # estimated resolution in degrees (from previous training)\n",
    "    sigma_q_over_pt = 0.011    # placeholder\n",
    "    sigma_tanl = 0.011    # placeholder\n",
    "    sigma_z   = 0.100  # estimated resolution in cm (from previous training)\n",
    "    \n",
    "    # These need to be replaced by custom loss functions that can calculate\n",
    "    # the sigmas based on the state vector.\n",
    "    losses = {\n",
    "        'D_output'         :  'mean_squared_error',\n",
    "        'phi_output'       :  'mean_squared_error',\n",
    "        'q_over_pt_output' :  'mean_squared_error',\n",
    "        'tanl_output'      :  'mean_squared_error',\n",
    "        'z_output'         :  'mean_squared_error',\n",
    "        }\n",
    "    lossWeights = {\n",
    "        'D_output'         :  1.0/(sigma_D*sigma_D),\n",
    "        'phi_output'       :  1.0/(sigma_phi*sigma_phi),\n",
    "        'q_over_pt_output' :  1.0/(sigma_q_over_pt*sigma_q_over_pt),\n",
    "        'tanl_output'      :  1.0/(sigma_tanl*sigma_tanl),\n",
    "        'z_output'         :  1.0/(sigma_z*sigma_z),\n",
    "    }\n",
    "    \n",
    "    outNames = ['D_output', 'phi_output', 'q_over_pt_output', 'tanl_output', 'z_output']  # names in order of model outputs\n",
    "    lossWeights_list = []\n",
    "    for n in outNames: lossWeights_list.append(lossWeights[n])\n",
    "\n",
    "    # Compile the model, possibly using multiple GPUs\n",
    "    opt = Adadelta(clipnorm=1.0)\n",
    "    if GPUS<=1 :\n",
    "        final_model = model\n",
    "    else:\n",
    "        final_model = multi_gpu_model( model, gpus=GPUS )\n",
    "\n",
    "    final_model.compile(loss='mse', loss_weights=[lossWeights_list], optimizer=opt, metrics=['mae', 'mse', 'accuracy'])\n",
    "    #final_model.compile(loss=losses, loss_weights=lossWeights, optimizer=opt, metrics=['mae', 'mse', 'accuracy'])\n",
    "    \n",
    "    return final_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find saved model. Will start from scratch\n",
      "MyWeightedAvg:\n",
      "       binsize = 0.400000\n",
      "          xmin = -12.000000\n",
      "   input shape = (?, 60)\n",
      "  output shape = (?, 1)\n",
      "MyWeightedAvg:\n",
      "       binsize = 0.400000\n",
      "          xmin = -12.000000\n",
      "   input shape = (?, 60)\n",
      "  output shape = (?, 1)\n",
      "MyWeightedAvg:\n",
      "       binsize = 0.400000\n",
      "          xmin = -12.000000\n",
      "   input shape = (?, 60)\n",
      "  output shape = (?, 1)\n",
      "MyWeightedAvg:\n",
      "       binsize = 0.333333\n",
      "          xmin = -10.000000\n",
      "   input shape = (?, 60)\n",
      "  output shape = (?, 1)\n",
      "MyWeightedAvg:\n",
      "       binsize = 0.333333\n",
      "          xmin = -10.000000\n",
      "   input shape = (?, 60)\n",
      "  output shape = (?, 1)\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_inputs (InputLayer)       [(None, 5835)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "top_layer1 (Flatten)            (None, 5835)         0           image_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "common_layer1 (Dense)           (None, 300)          1750800     top_layer1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "phi_output_dist (Dense)         (None, 60)           18060       common_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "D_output_dist (Dense)           (None, 60)           18060       common_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_over_pt_output_dist (Dense)   (None, 60)           18060       common_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tanl_output_dist (Dense)        (None, 60)           18060       common_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_output_dist (Dense)           (None, 60)           18060       common_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "phi_output (Lambda)             (None, 1)            0           phi_output_dist[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "D_output (Lambda)               (None, 1)            0           D_output_dist[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_over_pt_output (Lambda)       (None, 1)            0           q_over_pt_output_dist[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tanl_output (Lambda)            (None, 1)            0           tanl_output_dist[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_output (Lambda)               (None, 1)            0           z_output_dist[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 5)            0           phi_output[0][0]                 \n",
      "                                                                 D_output[0][0]                   \n",
      "                                                                 q_over_pt_output[0][0]           \n",
      "                                                                 tanl_output[0][0]                \n",
      "                                                                 z_output[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,841,100\n",
      "Trainable params: 1,841,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_inputs (InputLayer)       [(None, 5835)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "top_layer1 (Flatten)            (None, 5835)         0           image_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "common_layer1 (Dense)           (None, 300)          1750800     top_layer1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "phi_output_dist (Dense)         (None, 60)           18060       common_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "D_output_dist (Dense)           (None, 60)           18060       common_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_over_pt_output_dist (Dense)   (None, 60)           18060       common_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tanl_output_dist (Dense)        (None, 60)           18060       common_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_output_dist (Dense)           (None, 60)           18060       common_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "phi_output (Lambda)             (None, 1)            0           phi_output_dist[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "D_output (Lambda)               (None, 1)            0           D_output_dist[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_over_pt_output (Lambda)       (None, 1)            0           q_over_pt_output_dist[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tanl_output (Lambda)            (None, 1)            0           tanl_output_dist[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_output (Lambda)               (None, 1)            0           z_output_dist[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 5)            0           phi_output[0][0]                 \n",
      "                                                                 D_output[0][0]                   \n",
      "                                                                 q_over_pt_output[0][0]           \n",
      "                                                                 tanl_output[0][0]                \n",
      "                                                                 z_output[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,841,100\n",
      "Trainable params: 1,841,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Here we want to check if a model has been saved due to previous training.\n",
    "# If so, then we read it in and continue training where it left off. Otherwise,\n",
    "# we define the model and start fresh. \n",
    "\n",
    "# Look for most recent saved epoch\n",
    "epoch_loaded = -1\n",
    "if os.path.exists('model_checkpoints'):\n",
    "    for f in os.listdir('model_checkpoints'):\n",
    "        if f.startswith('model_epoch') and f.endswith('.h5'):\n",
    "            e = int(f[11:-3])\n",
    "            if e > epoch_loaded:\n",
    "                epoch_loaded = e\n",
    "                fname = 'model_checkpoints/model_epoch%03d.h5' % epoch_loaded\n",
    "\n",
    "if epoch_loaded > 0:\n",
    "    print('Loading model: ' + fname)\n",
    "    model = load_model( fname )\n",
    "else:\n",
    "    print('Unable to find saved model. Will start from scratch')\n",
    "    model = DefineModel()\n",
    "    epoch_loaded = 0\n",
    "\n",
    "# Print summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model: model_checkpoints/model_epoch000.h5\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/3\n",
      "removing old model: model_checkpoints/model_epoch000.h5\n",
      "saving model: model_checkpoints/model_epoch001.h5\n",
      "800/800 [==============================] - 2s 2ms/sample - loss: 41724844.0000 - mean_absolute_error: 26.0426 - mean_squared_error: 6291.8501 - acc: 0.0000e+00 - val_loss: 14135397.0000 - val_mean_absolute_error: 21.9477 - val_mean_squared_error: 2131.5310 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "removing old model: model_checkpoints/model_epoch001.h5\n",
      "saving model: model_checkpoints/model_epoch002.h5\n",
      "800/800 [==============================] - 0s 462us/sample - loss: 41709800.0000 - mean_absolute_error: 26.0233 - mean_squared_error: 6289.5806 - acc: 0.0000e+00 - val_loss: 14122118.0000 - val_mean_absolute_error: 21.9283 - val_mean_squared_error: 2129.5286 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "removing old model: model_checkpoints/model_epoch002.h5\n",
      "saving model: model_checkpoints/model_epoch003.h5\n",
      "800/800 [==============================] - 0s 457us/sample - loss: 41694656.0000 - mean_absolute_error: 26.0039 - mean_squared_error: 6287.2969 - acc: 0.0000e+00 - val_loss: 14108786.0000 - val_mean_absolute_error: 21.9088 - val_mean_squared_error: 2127.5181 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------\n",
    "# class checkpointModel\n",
    "#-----------------------------------------------------\n",
    "# There is a bug in keras that causes an error when trying to save a model\n",
    "# trained on multiple GPUs. The work around is to save the original model\n",
    "# at the end of every epoch using a callback. See\n",
    "#    https://github.com/keras-team/kersas/issues/8694\n",
    "if not os.path.exists('model_checkpoints'): os.mkdir('model_checkpoints')\n",
    "class checkpointModel(Callback):\n",
    "    def __init__(self, model):\n",
    "        self.model_to_save = model\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        myepoch = epoch_loaded + epoch +1\n",
    "        fname = 'model_checkpoints/model_epoch%03d.h5' % myepoch\n",
    "        old_fname = 'model_checkpoints/model_epoch%03d.h5' % (myepoch-1)\n",
    "        if os.path.exists( old_fname ):\n",
    "            print('removing old model: %s' % old_fname)\n",
    "            os.remove( old_fname )\n",
    "        print('saving model: %s' % fname)\n",
    "        self.model_to_save.save(fname)\n",
    "cbk = checkpointModel( model )\n",
    "\n",
    "cbk.on_epoch_end(-1)\n",
    "\n",
    "EPOCHS = 3\n",
    "BS     = 1000\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x = df,\n",
    "    y = labelsdf.iloc[:, 1:6],  # Peel off only 5 state vector parameters\n",
    "    batch_size = BS,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[cbk],\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    initial_epoch = epoch_loaded,\n",
    "    use_multiprocessing=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
