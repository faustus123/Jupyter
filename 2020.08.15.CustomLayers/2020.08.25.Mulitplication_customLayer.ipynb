{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplying 2 numbers with a custom layer\n",
    "\n",
    "In this notebook I create a simple 3-node network that multiplies 2 numbers. This implements an actual multiplication operation in a custom layer (as opposed to mimicing multiplication with a deep network). To make the layer more flexible, I give it trainable weights that are used as exponents for each of the input values. Thus, the single output from the layer is given by:\n",
    "\n",
    "$$\n",
    "y = b + \\prod_{i=0}^{N-1} x_{i}^{w_{i}}\n",
    "$$\n",
    "\n",
    "where:<br>\n",
    "$b$ is the bias (which I fix at 0 for this example)<br>\n",
    "$x_{i}$ are the inputs<br>\n",
    "$w_{i}$ are the weights<br>\n",
    "\n",
    "The custom layer accepts arguments for the number of inputs, an optional \"trainable\" argument for whether the weights should be adjusted during the training of the full model, and another optional \"initial_exponent\" argument so the caller can set the initial value of the exponents (i.e. weights). This is all done in the ProductLayer class defined in the following cell.\n",
    "\n",
    "In addition to defining the custom layer, I define a simple model that uses it and then tests it with a small set of inputs right at the end of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape=(None, 2)\n",
      "inputs.shape: (None, 2)\n",
      "self.w.shape: (1, 2)\n",
      "   tmp.shape: (None, 2)\n",
      " myout.shape: (None, 1)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "product_layer (ProductLayer) (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 2\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n",
      "inputs.shape: (3, 2)\n",
      "self.w.shape: (1, 2)\n",
      "   tmp.shape: (3, 2)\n",
      " myout.shape: (3, 1)\n",
      "tf.Tensor(\n",
      "[[82.810524]\n",
      " [82.810524]\n",
      " [82.810524]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "NINPUTS = 2\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# ProductLayer\n",
    "#-----------------------------------------------------\n",
    "# This defines a layer that takes the product of the inputs,\n",
    "# each raised to the power of its weight. The trainable\n",
    "# parameter can be set to False to make it non-trainable.\n",
    "# n.b. If you make this trainable, the inputs cannot be\n",
    "# negative numbers!\n",
    "# See details on this in the following cell.\n",
    "class ProductLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=1, trainable=True, initial_exponent=2.01):\n",
    "        super(ProductLayer, self).__init__()\n",
    "        self.units            = units\n",
    "        self.trainable        = trainable\n",
    "        self.initial_exponent = initial_exponent\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        print('input_shape='+str(input_shape))\n",
    "        myinitializer   = tf.keras.initializers.Constant(self.initial_exponent)\n",
    "        self.w = self.add_weight(\n",
    "            shape       = (self.units, input_shape[-1]),\n",
    "            initializer = myinitializer,\n",
    "            trainable   = self.trainable,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape       = (self.units,),\n",
    "            initializer = \"zeros\",\n",
    "            trainable   = False\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs has shape (None, 2)\n",
    "        # self.w has shape (1, 2)\n",
    "        # tmp has shape (None, 2)\n",
    "        # output has shape (None, 1)\n",
    "        tmp = K.pow(inputs, self.w)\n",
    "        myout = K.prod(tmp, keepdims=True, axis=1) + self.b\n",
    "        print('inputs.shape: ' + str(inputs.shape))\n",
    "        print('self.w.shape: ' + str(self.w.shape))\n",
    "        print('   tmp.shape: ' + str(tmp.shape))\n",
    "        print(' myout.shape: ' + str(myout.shape))\n",
    "        return myout\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ProductLayer, self).get_config()\n",
    "        config.update({\"units\": self.units, \"trainable\": self.trainable, \"initial_exponent\": self.initial_exponent})\n",
    "        return config\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# DefineModel\n",
    "#-----------------------------------------------------\n",
    "# This is used to define the model. It is only called if no model\n",
    "# file is found in the model_checkpoints directory.\n",
    "def DefineModel():\n",
    "\n",
    "    # Build the network model with 2 inputs and one output.\n",
    "    inputs = Input(shape=(NINPUTS,), name='inputs')\n",
    "    output = ProductLayer(1)(inputs)\n",
    "    model  = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    opt = Adadelta(clipnorm=1.0)\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=['mae', 'mse', 'accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = DefineModel()\n",
    "model.summary()\n",
    "\n",
    "x = tf.ones((3,2), dtype=tf.dtypes.float32)*(3.0001)\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown of ProductLayer\n",
    "\n",
    "The ProductLayer class implements a custom layer in Keras. It has 4 methods described below. \n",
    "\n",
    "1. **\\_\\_init\\_\\_()**: This is the standard python constructor which gets called when the object is created. If there are any options given when the object is instantiated, they will be passed in here. This basically needs to save them as part of the object so they can be used in the other callback methods where the actual work is done.\n",
    "\n",
    "2. **build()**: This is called automatically the first time the call() method is called. This is all handled by Keras as it is what actually calls \"call()\" and recogonizes it needs to call \"build()\" first. This method is responsible for declaring any \"weights\" in the layer. I quote \"weights\" since you may notice that the same add_weight() method is called to add the bias values. It is really a way of declaring to Keras the trainable values in the layer. The build() method is actually optional since you could define a layer that does not have any trainable weights. Note that in this example I initialize the bias weights to 0 and then set them as non-trainable. This really is really overkill as I could get the same affect by not adding any bias weights at all. I left it in though to emphasize that any number of \"weights\" could be added here.\n",
    "\n",
    "3. **call()**: This is called to create an output Tensor (with a capital-T) based on some given inputs. This is where I actually implement what math the layer will do. This is only called once when the model is compiled. It uses Keras backend functions to define the set of operations that should be performed on the inputs in order to produce the outputs. It does not actually perform those operations during the call. Since Keras+Tensorflow know the operations, it also knows their derivatives which it can chain together to backpropagate during training. Most of my time here is spent on checking the shapes of the inputs to each operation to make sure the automatic looping over nodes is done correctly. See more details below.\n",
    "\n",
    "4. **config()**: This is needed when saving the model so it can get the parameters needed to configure the layer when it is loaded later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Explanation of lines in call()\n",
    "  \n",
    "  The call method() has only two lines of real content that define the operation. The only important thing in the *inputs* valriable is its shape. In the comments the shapes of the various variables are given. Some of these contain *None* which acts as a placeholder. When the training is done and an actual set of values is given, the *None* value will be replaced by the batch size. Thus, the *(None, 2)* shape of *inputs* indicates some arbitrary number of sets of inputs with each set containing 2 numbers. The *2* is because we defined the layer to have only 2 inputs in the model (see NINPUTS=2 at top). This meaningful line is:\n",
    "  \n",
    "    tmp = K.pow(inputs, self.w)\n",
    "\n",
    "This line takes the inputs and raises them to the powers given by the weights. Since this is a backend function, it will automatically do this for all sets of inputs in the batch. The variable *tmp* therefore has a shape *(None, 2)* where again, *None* is the placeholder for the batch size.\n",
    "\n",
    "    myout = K.prod(tmp, keepdims=True, axis=1) + self.b\n",
    "\n",
    "This line takes the product of each value in a set and then adds the bias. The two arguments *keepDims=True* and *axis=1* say not to multiply **ALL** of the numbers together, but only those within the same batch element. Specifically, only multiply values on the *1-th* axis and not on the *0-th* axis. The the output of the K.prod() call will have a shape of *(None, 1)*. The shape of self.b does not ha\n",
    "\n",
    "ve a *None* dimension, but Keras is smart enough to know that you want to add this one number to all of the values in the *(None,1)* Tensor returned by K.prod().\n",
    "\n",
    "The value of *myout* is returned and indeed, you can see that the output shape of the model summary is *(None, 1)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "Below is some pretty standard code for generating a set of inputs(aka *features*) and labels. Note that for this example, the range of values of the inputs are all positive. This is because I allow the weights to be trained as floating point numbers and taking a negative number to a fractional power is undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Generate dataframes for features and labels\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "Z = []\n",
    "for x in np.arange(0.0, 10.1, 0.1):\n",
    "    for y in np.arange(0.0, 10.1, 0.1):\n",
    "        z = x*y\n",
    "        X.append([x,y])  # features\n",
    "        Z.append([z])    # labels\n",
    "\n",
    "df = pd.DataFrame(X, columns=['x', 'y'])\n",
    "labelsdf = pd.DataFrame(Z, columns=['z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10201/10201 [==============================] - 5s 534us/step - loss: 2.7589e-09 - mae: 1.9170e-05 - mse: 2.7589e-09 - accuracy: 0.0206\n",
      "Epoch 2/100\n",
      "10201/10201 [==============================] - 5s 533us/step - loss: 2.8945e-09 - mae: 2.0573e-05 - mse: 2.8945e-09 - accuracy: 0.0206\n",
      "Epoch 3/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.8919e-09 - mae: 2.0276e-05 - mse: 2.8919e-09 - accuracy: 0.0206\n",
      "Epoch 4/100\n",
      "10201/10201 [==============================] - 5s 530us/step - loss: 2.8700e-09 - mae: 1.9884e-05 - mse: 2.8700e-09 - accuracy: 0.0206\n",
      "Epoch 5/100\n",
      "10201/10201 [==============================] - 5s 538us/step - loss: 2.7174e-09 - mae: 1.9748e-05 - mse: 2.7174e-09 - accuracy: 0.0206\n",
      "Epoch 6/100\n",
      "10201/10201 [==============================] - 5s 537us/step - loss: 2.8062e-09 - mae: 1.8667e-05 - mse: 2.8062e-09 - accuracy: 0.0206\n",
      "Epoch 7/100\n",
      "10201/10201 [==============================] - 5s 537us/step - loss: 3.0572e-09 - mae: 2.1282e-05 - mse: 3.0572e-09 - accuracy: 0.0206\n",
      "Epoch 8/100\n",
      "10201/10201 [==============================] - 5s 531us/step - loss: 2.8599e-09 - mae: 2.0722e-05 - mse: 2.8599e-09 - accuracy: 0.0206\n",
      "Epoch 9/100\n",
      "10201/10201 [==============================] - 5s 530us/step - loss: 3.0005e-09 - mae: 2.1252e-05 - mse: 3.0005e-09 - accuracy: 0.0206\n",
      "Epoch 10/100\n",
      "10201/10201 [==============================] - 5s 534us/step - loss: 3.2624e-09 - mae: 2.0132e-05 - mse: 3.2624e-09 - accuracy: 0.0206\n",
      "Epoch 11/100\n",
      "10201/10201 [==============================] - 5s 534us/step - loss: 2.6855e-09 - mae: 1.8970e-05 - mse: 2.6855e-09 - accuracy: 0.0206\n",
      "Epoch 12/100\n",
      "10201/10201 [==============================] - 5s 531us/step - loss: 2.8629e-09 - mae: 1.9441e-05 - mse: 2.8629e-09 - accuracy: 0.0206\n",
      "Epoch 13/100\n",
      "10201/10201 [==============================] - 5s 537us/step - loss: 2.6472e-09 - mae: 1.9150e-05 - mse: 2.6472e-09 - accuracy: 0.0206\n",
      "Epoch 14/100\n",
      "10201/10201 [==============================] - 5s 531us/step - loss: 2.6234e-09 - mae: 1.8939e-05 - mse: 2.6234e-09 - accuracy: 0.0206\n",
      "Epoch 15/100\n",
      "10201/10201 [==============================] - 5s 536us/step - loss: 2.9768e-09 - mae: 1.9811e-05 - mse: 2.9768e-09 - accuracy: 0.0206\n",
      "Epoch 16/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.9675e-09 - mae: 2.0196e-05 - mse: 2.9675e-09 - accuracy: 0.0206\n",
      "Epoch 17/100\n",
      "10201/10201 [==============================] - 5s 531us/step - loss: 2.7114e-09 - mae: 1.9694e-05 - mse: 2.7114e-09 - accuracy: 0.0206\n",
      "Epoch 18/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.9194e-09 - mae: 2.0651e-05 - mse: 2.9194e-09 - accuracy: 0.0206\n",
      "Epoch 19/100\n",
      "10201/10201 [==============================] - 5s 527us/step - loss: 3.0577e-09 - mae: 2.0782e-05 - mse: 3.0577e-09 - accuracy: 0.0206\n",
      "Epoch 20/100\n",
      "10201/10201 [==============================] - 5s 529us/step - loss: 2.6950e-09 - mae: 1.9924e-05 - mse: 2.6950e-09 - accuracy: 0.0206\n",
      "Epoch 21/100\n",
      "10201/10201 [==============================] - 5s 529us/step - loss: 2.7526e-09 - mae: 1.9747e-05 - mse: 2.7526e-09 - accuracy: 0.0206\n",
      "Epoch 22/100\n",
      "10201/10201 [==============================] - 5s 531us/step - loss: 2.8396e-09 - mae: 2.0266e-05 - mse: 2.8396e-09 - accuracy: 0.0206\n",
      "Epoch 23/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 3.1594e-09 - mae: 2.0488e-05 - mse: 3.1594e-09 - accuracy: 0.0206\n",
      "Epoch 24/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.8075e-09 - mae: 1.9583e-05 - mse: 2.8075e-09 - accuracy: 0.0206\n",
      "Epoch 25/100\n",
      "10201/10201 [==============================] - 5s 534us/step - loss: 2.6317e-09 - mae: 1.9456e-05 - mse: 2.6317e-09 - accuracy: 0.0206\n",
      "Epoch 26/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.9314e-09 - mae: 2.0197e-05 - mse: 2.9314e-09 - accuracy: 0.0206\n",
      "Epoch 27/100\n",
      "10201/10201 [==============================] - 6s 542us/step - loss: 3.0969e-09 - mae: 2.0022e-05 - mse: 3.0969e-09 - accuracy: 0.0206\n",
      "Epoch 28/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.9007e-09 - mae: 1.9826e-05 - mse: 2.9007e-09 - accuracy: 0.0206\n",
      "Epoch 29/100\n",
      "10201/10201 [==============================] - 5s 532us/step - loss: 2.6681e-09 - mae: 1.9428e-05 - mse: 2.6681e-09 - accuracy: 0.0206\n",
      "Epoch 30/100\n",
      "10201/10201 [==============================] - 5s 531us/step - loss: 2.5771e-09 - mae: 1.8782e-05 - mse: 2.5771e-09 - accuracy: 0.0206\n",
      "Epoch 31/100\n",
      "10201/10201 [==============================] - 5s 539us/step - loss: 2.9204e-09 - mae: 2.0110e-05 - mse: 2.9204e-09 - accuracy: 0.0206\n",
      "Epoch 32/100\n",
      "10201/10201 [==============================] - 5s 534us/step - loss: 3.0394e-09 - mae: 2.0956e-05 - mse: 3.0394e-09 - accuracy: 0.0206\n",
      "Epoch 33/100\n",
      "10201/10201 [==============================] - 5s 537us/step - loss: 2.6269e-09 - mae: 1.9532e-05 - mse: 2.6269e-09 - accuracy: 0.0206\n",
      "Epoch 34/100\n",
      "10201/10201 [==============================] - 5s 537us/step - loss: 2.7875e-09 - mae: 1.9609e-05 - mse: 2.7875e-09 - accuracy: 0.0206\n",
      "Epoch 35/100\n",
      "10201/10201 [==============================] - 5s 531us/step - loss: 2.7919e-09 - mae: 1.9787e-05 - mse: 2.7919e-09 - accuracy: 0.0206\n",
      "Epoch 36/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.7599e-09 - mae: 1.9878e-05 - mse: 2.7599e-09 - accuracy: 0.0206\n",
      "Epoch 37/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.7246e-09 - mae: 1.9825e-05 - mse: 2.7246e-09 - accuracy: 0.0206\n",
      "Epoch 38/100\n",
      "10201/10201 [==============================] - 6s 540us/step - loss: 3.0026e-09 - mae: 2.0861e-05 - mse: 3.0026e-09 - accuracy: 0.0206\n",
      "Epoch 39/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.7141e-09 - mae: 2.0129e-05 - mse: 2.7141e-09 - accuracy: 0.0206\n",
      "Epoch 40/100\n",
      "10201/10201 [==============================] - 5s 536us/step - loss: 2.7609e-09 - mae: 2.0107e-05 - mse: 2.7609e-09 - accuracy: 0.0206\n",
      "Epoch 41/100\n",
      "10201/10201 [==============================] - 5s 534us/step - loss: 2.8143e-09 - mae: 2.0425e-05 - mse: 2.8143e-09 - accuracy: 0.0206\n",
      "Epoch 42/100\n",
      "10201/10201 [==============================] - 5s 533us/step - loss: 2.9793e-09 - mae: 1.9835e-05 - mse: 2.9793e-09 - accuracy: 0.0206\n",
      "Epoch 43/100\n",
      "10201/10201 [==============================] - 5s 528us/step - loss: 2.8128e-09 - mae: 1.9686e-05 - mse: 2.8128e-09 - accuracy: 0.0206\n",
      "Epoch 44/100\n",
      "10201/10201 [==============================] - 6s 544us/step - loss: 2.7093e-09 - mae: 1.9951e-05 - mse: 2.7093e-09 - accuracy: 0.0206\n",
      "Epoch 45/100\n",
      "10201/10201 [==============================] - 5s 528us/step - loss: 2.5974e-09 - mae: 1.8967e-05 - mse: 2.5974e-09 - accuracy: 0.0206\n",
      "Epoch 46/100\n",
      "10201/10201 [==============================] - 5s 529us/step - loss: 2.7176e-09 - mae: 1.9003e-05 - mse: 2.7176e-09 - accuracy: 0.0206\n",
      "Epoch 47/100\n",
      "10201/10201 [==============================] - 5s 532us/step - loss: 2.7832e-09 - mae: 1.9888e-05 - mse: 2.7832e-09 - accuracy: 0.0206\n",
      "Epoch 48/100\n",
      "10201/10201 [==============================] - 6s 586us/step - loss: 2.7316e-09 - mae: 1.8902e-05 - mse: 2.7316e-09 - accuracy: 0.0206\n",
      "Epoch 49/100\n",
      "10201/10201 [==============================] - 6s 587us/step - loss: 3.0629e-09 - mae: 1.9643e-05 - mse: 3.0629e-09 - accuracy: 0.0206\n",
      "Epoch 50/100\n",
      "10201/10201 [==============================] - 6s 591us/step - loss: 2.6385e-09 - mae: 1.9451e-05 - mse: 2.6385e-09 - accuracy: 0.0206\n",
      "Epoch 51/100\n",
      "10201/10201 [==============================] - 6s 588us/step - loss: 2.6548e-09 - mae: 1.9604e-05 - mse: 2.6548e-09 - accuracy: 0.0206\n",
      "Epoch 52/100\n",
      "10201/10201 [==============================] - 6s 584us/step - loss: 2.9738e-09 - mae: 1.9961e-05 - mse: 2.9738e-09 - accuracy: 0.0206\n",
      "Epoch 53/100\n",
      "10201/10201 [==============================] - 6s 590us/step - loss: 2.8245e-09 - mae: 1.9782e-05 - mse: 2.8245e-09 - accuracy: 0.0206\n",
      "Epoch 54/100\n",
      "10201/10201 [==============================] - 6s 586us/step - loss: 2.6577e-09 - mae: 1.9328e-05 - mse: 2.6577e-09 - accuracy: 0.0206\n",
      "Epoch 55/100\n",
      "10201/10201 [==============================] - 6s 586us/step - loss: 2.8947e-09 - mae: 1.9932e-05 - mse: 2.8947e-09 - accuracy: 0.0206\n",
      "Epoch 56/100\n",
      "10201/10201 [==============================] - 6s 584us/step - loss: 2.7955e-09 - mae: 1.9517e-05 - mse: 2.7955e-09 - accuracy: 0.0206\n",
      "Epoch 57/100\n",
      "10201/10201 [==============================] - 6s 584us/step - loss: 2.7842e-09 - mae: 1.9664e-05 - mse: 2.7842e-09 - accuracy: 0.0206\n",
      "Epoch 58/100\n",
      "10201/10201 [==============================] - 6s 593us/step - loss: 2.6456e-09 - mae: 1.9868e-05 - mse: 2.6456e-09 - accuracy: 0.0206\n",
      "Epoch 59/100\n",
      "10201/10201 [==============================] - 6s 587us/step - loss: 2.6538e-09 - mae: 2.0704e-05 - mse: 2.6538e-09 - accuracy: 0.0206\n",
      "Epoch 60/100\n",
      "10201/10201 [==============================] - 6s 540us/step - loss: 2.8811e-09 - mae: 2.0803e-05 - mse: 2.8811e-09 - accuracy: 0.0206\n",
      "Epoch 61/100\n",
      "10201/10201 [==============================] - 5s 536us/step - loss: 2.9295e-09 - mae: 1.9963e-05 - mse: 2.9295e-09 - accuracy: 0.0206\n",
      "Epoch 62/100\n",
      "10201/10201 [==============================] - 5s 533us/step - loss: 3.0172e-09 - mae: 2.0537e-05 - mse: 3.0172e-09 - accuracy: 0.0206\n",
      "Epoch 63/100\n",
      "10201/10201 [==============================] - 5s 536us/step - loss: 2.6193e-09 - mae: 1.9196e-05 - mse: 2.6193e-09 - accuracy: 0.0206\n",
      "Epoch 64/100\n",
      "10201/10201 [==============================] - 5s 533us/step - loss: 2.9807e-09 - mae: 2.0054e-05 - mse: 2.9807e-09 - accuracy: 0.0206\n",
      "Epoch 65/100\n",
      "10201/10201 [==============================] - 5s 536us/step - loss: 3.1128e-09 - mae: 1.9772e-05 - mse: 3.1128e-09 - accuracy: 0.0206\n",
      "Epoch 66/100\n",
      "10201/10201 [==============================] - 5s 538us/step - loss: 2.4824e-09 - mae: 1.8799e-05 - mse: 2.4824e-09 - accuracy: 0.0206\n",
      "Epoch 67/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.8994e-09 - mae: 2.0055e-05 - mse: 2.8994e-09 - accuracy: 0.0206\n",
      "Epoch 68/100\n",
      "10201/10201 [==============================] - 5s 529us/step - loss: 2.6539e-09 - mae: 1.9892e-05 - mse: 2.6539e-09 - accuracy: 0.0206\n",
      "Epoch 69/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.8984e-09 - mae: 2.0084e-05 - mse: 2.8984e-09 - accuracy: 0.0206\n",
      "Epoch 70/100\n",
      "10201/10201 [==============================] - 5s 534us/step - loss: 2.9608e-09 - mae: 1.9440e-05 - mse: 2.9608e-09 - accuracy: 0.0206\n",
      "Epoch 71/100\n",
      "10201/10201 [==============================] - 5s 538us/step - loss: 2.7877e-09 - mae: 2.0357e-05 - mse: 2.7877e-09 - accuracy: 0.0206\n",
      "Epoch 72/100\n",
      "10201/10201 [==============================] - 6s 540us/step - loss: 2.8477e-09 - mae: 2.0167e-05 - mse: 2.8477e-09 - accuracy: 0.0206\n",
      "Epoch 73/100\n",
      "10201/10201 [==============================] - 5s 532us/step - loss: 2.7544e-09 - mae: 1.9505e-05 - mse: 2.7544e-09 - accuracy: 0.0206\n",
      "Epoch 74/100\n",
      "10201/10201 [==============================] - 6s 541us/step - loss: 2.8592e-09 - mae: 1.9970e-05 - mse: 2.8592e-09 - accuracy: 0.0206\n",
      "Epoch 75/100\n",
      "10201/10201 [==============================] - 5s 531us/step - loss: 2.4464e-09 - mae: 1.8473e-05 - mse: 2.4464e-09 - accuracy: 0.0206\n",
      "Epoch 76/100\n",
      "10201/10201 [==============================] - 6s 543us/step - loss: 2.5921e-09 - mae: 1.9628e-05 - mse: 2.5921e-09 - accuracy: 0.0206\n",
      "Epoch 77/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.6807e-09 - mae: 2.0327e-05 - mse: 2.6807e-09 - accuracy: 0.0206\n",
      "Epoch 78/100\n",
      "10201/10201 [==============================] - 6s 543us/step - loss: 2.7103e-09 - mae: 1.9532e-05 - mse: 2.7103e-09 - accuracy: 0.0206\n",
      "Epoch 79/100\n",
      "10201/10201 [==============================] - 5s 538us/step - loss: 2.6947e-09 - mae: 1.9467e-05 - mse: 2.6947e-09 - accuracy: 0.0206\n",
      "Epoch 80/100\n",
      "10201/10201 [==============================] - 5s 536us/step - loss: 2.9500e-09 - mae: 2.0996e-05 - mse: 2.9500e-09 - accuracy: 0.0206\n",
      "Epoch 81/100\n",
      "10201/10201 [==============================] - 5s 530us/step - loss: 3.0143e-09 - mae: 2.0152e-05 - mse: 3.0143e-09 - accuracy: 0.0206\n",
      "Epoch 82/100\n",
      "10201/10201 [==============================] - 5s 536us/step - loss: 2.9126e-09 - mae: 1.9918e-05 - mse: 2.9126e-09 - accuracy: 0.0206\n",
      "Epoch 83/100\n",
      "10201/10201 [==============================] - 5s 530us/step - loss: 2.6046e-09 - mae: 1.9546e-05 - mse: 2.6046e-09 - accuracy: 0.0206\n",
      "Epoch 84/100\n",
      "10201/10201 [==============================] - 5s 536us/step - loss: 2.8009e-09 - mae: 2.0147e-05 - mse: 2.8009e-09 - accuracy: 0.0206\n",
      "Epoch 85/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.9274e-09 - mae: 2.0085e-05 - mse: 2.9274e-09 - accuracy: 0.0206\n",
      "Epoch 86/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.6753e-09 - mae: 1.9771e-05 - mse: 2.6753e-09 - accuracy: 0.0206\n",
      "Epoch 87/100\n",
      "10201/10201 [==============================] - 5s 534us/step - loss: 2.7742e-09 - mae: 2.0720e-05 - mse: 2.7742e-09 - accuracy: 0.0206\n",
      "Epoch 88/100\n",
      "10201/10201 [==============================] - 5s 536us/step - loss: 2.6052e-09 - mae: 1.9533e-05 - mse: 2.6052e-09 - accuracy: 0.0206\n",
      "Epoch 89/100\n",
      "10201/10201 [==============================] - 5s 535us/step - loss: 2.8987e-09 - mae: 2.1506e-05 - mse: 2.8987e-09 - accuracy: 0.0206\n",
      "Epoch 90/100\n",
      "10201/10201 [==============================] - 6s 588us/step - loss: 3.0778e-09 - mae: 1.9537e-05 - mse: 3.0778e-09 - accuracy: 0.0206\n",
      "Epoch 91/100\n",
      "10201/10201 [==============================] - 6s 583us/step - loss: 3.1163e-09 - mae: 2.0173e-05 - mse: 3.1163e-09 - accuracy: 0.0206\n",
      "Epoch 92/100\n",
      "10201/10201 [==============================] - 6s 586us/step - loss: 2.6793e-09 - mae: 1.8979e-05 - mse: 2.6793e-09 - accuracy: 0.0206\n",
      "Epoch 93/100\n",
      "10201/10201 [==============================] - 6s 585us/step - loss: 2.8538e-09 - mae: 1.9345e-05 - mse: 2.8538e-09 - accuracy: 0.0206\n",
      "Epoch 94/100\n",
      "10201/10201 [==============================] - 6s 584us/step - loss: 2.8233e-09 - mae: 2.0052e-05 - mse: 2.8233e-09 - accuracy: 0.0206\n",
      "Epoch 95/100\n",
      "10201/10201 [==============================] - 6s 578us/step - loss: 2.6994e-09 - mae: 1.9458e-05 - mse: 2.6994e-09 - accuracy: 0.0206\n",
      "Epoch 96/100\n",
      "10201/10201 [==============================] - 6s 587us/step - loss: 2.8290e-09 - mae: 2.0327e-05 - mse: 2.8290e-09 - accuracy: 0.02060s - loss: 2.8363e-09 - mae: 2.0314e-05 - mse: 2.8363e-09 - accuracy: 0.\n",
      "Epoch 97/100\n",
      "10201/10201 [==============================] - 6s 587us/step - loss: 2.6667e-09 - mae: 1.9552e-05 - mse: 2.6667e-09 - accuracy: 0.0206\n",
      "Epoch 98/100\n",
      "10201/10201 [==============================] - 6s 580us/step - loss: 2.6423e-09 - mae: 1.9127e-05 - mse: 2.6423e-09 - accuracy: 0.0206\n",
      "Epoch 99/100\n",
      "10201/10201 [==============================] - 6s 582us/step - loss: 2.9537e-09 - mae: 1.9548e-05 - mse: 2.9537e-09 - accuracy: 0.0206\n",
      "Epoch 100/100\n",
      "10201/10201 [==============================] - 6s 584us/step - loss: 2.7507e-09 - mae: 2.0205e-05 - mse: 2.7507e-09 - accuracy: 0.0206\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to create link (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d3ce1ac9a38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multiply_model_customLayer01.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/w/halld-scifs17exp/halld2/home/davidl/builds/Python_VENV/venv_2020.06.02/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \"\"\"\n\u001b[1;32m   1051\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1052\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/w/halld-scifs17exp/halld2/home/davidl/builds/Python_VENV/venv_2020.06.02/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    133\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    134\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 135\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    136\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m/w/halld-scifs17exp/halld2/home/davidl/builds/Python_VENV/venv_2020.06.02/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/w/halld-scifs17exp/halld2/home/davidl/builds/Python_VENV/venv_2020.06.02/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0msave_attributes_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m       \u001b[0mparam_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/w/halld-scifs17exp/halld2/home/davidl/builds/Python_VENV/venv_2020.06.02/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/w/halld-scifs17exp/halld2/home/davidl/builds/Python_VENV/venv_2020.06.02/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to create link (name already exists)"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100  # (in addition to anything already done)\n",
    "BS     = 1\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x = df,\n",
    "    y = labelsdf,\n",
    "    batch_size = BS,\n",
    "    epochs=EPOCHS,\n",
    "    #validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=False\n",
    ")\n",
    "\n",
    "model.save('multiply_model_customLayer01.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the result\n",
    "\n",
    "For this simple example, the exponents are initialized to 2.01, but the labels are generated assuming all are 1.0. This lets us verify that the training actually worked and found the correct values. Printing the weights below shows they both came out pretty close to 1.0. I should note that if I allow the bias to train as well, it will cause the training to take longer, but it will eventually get down close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers: print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An even simpler example using a Lambda layer\n",
    "\n",
    "The above is doing something pretty simple, but the definition of the ProductLayer class does seem kind of large for something that is essentially $x_{0}\\times x_{1}$. It has the benefit though of being something that could be expanded to a pretty complex formula. Suppose though that we wanted to instead create a model that did the same thing, but did not have any trainable weights. In this case we could use a Keras *Lambda* layer. For this, we just need to define a procedure that is essentially the *call()* method of *ProductLayer*.\n",
    "\n",
    "In the following cell, I define such a routine. Here, the exponents are hardcoded as 0.5, 2.0 just by way of example. This means the output of the layer will be the product of the square root of the first input and the square of the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "#-----------------------------------------------------\n",
    "# MyProductLambda\n",
    "#-----------------------------------------------------\n",
    "def MyProductLambda(inputs):\n",
    "    tmp = K.pow(inputs, (0.5, 2.0))\n",
    "    return K.prod(tmp, keepdims=True, axis=1)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# DefineModelLambda\n",
    "#-----------------------------------------------------\n",
    "def DefineModelLambda():\n",
    "\n",
    "    # Build the network model with 2 inputs and one output.\n",
    "    inputs = Input(shape=(NINPUTS,), name='inputs')\n",
    "    output = Lambda(MyProductLambda, output_shape=(1,))(inputs)\n",
    "    model  = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    opt = Adadelta(clipnorm=1.0)\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=['mae', 'mse', 'accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model_lambda = DefineModelLambda()\n",
    "model_lambda.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model. We give all of the inputs as \"4\" so sqrt(4)*4^2 = 32 which you can see \n",
    "x_lambda = tf.ones((3,2), dtype=tf.dtypes.float32)*(4.0)\n",
    "y_lambda = model_lambda(x_lambda)\n",
    "print(y_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_2020.06.02",
   "language": "python",
   "name": "venv_2020.06.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
